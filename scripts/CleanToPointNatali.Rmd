---
title: "Natali Clean to Point"
author: "Anna Talucci"
date: '2022-12-11'
output: html_document
---




# clear environment
```{r}
rm(list=ls())
```

# Overview
**MISSING SOME DAY OF MONTH but it does not matter because the thaw depth measures (n=732 from site_id=YKD) all have dates which are needed to standarize** **ERROR WHEN CREATING POINTS**
Adapt script form M. Loranty data_cleaning, to only clean and not aggregate data. Split data by active and thaw, add julian date for thaw, create point shapefile for each active and thaw.

Use data from csv files in Google Drive

What this script does:


# Packages

```{r}
library(tidyverse)
library(lubridate)
library(sf)
```
# Projection

WGS 84 need for gee
```{r}
wgs_proj =  "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs "
```

# Data

# vector of data files
```{r}
f <- list.files(path = "../data/GoogleDriveCsvFolder/",
                pattern = "*.csv", full.names = TRUE)
```

```{r}
f
```



## Natali
```{r}
f10 <- read.csv(f[10], header = TRUE)
f10
```

# fix incorrect logical columns
```{r}
# organic depth is missing, but set to NA
f10$organic_depth <- as.numeric(f10$organic_depth)

# fix incorrect logical columns
f10[,c(20:22)] <- read.csv(f[10], header = TRUE, colClasses = "character")[,c(20:22)]

# remove characters from thaw depth and convert to numeric
f10$thaw_depth <- gsub("+", "", f10$thaw_depth, fixed = TRUE)
f10$thaw_depth <- as.numeric(gsub(">", "", f10$thaw_depth, fixed = TRUE))
f10$long = as.numeric(gsub(",","",f10$long))
```

```{r}
str(f10)
```

```{r}
f10_dates = f10 %>%
  mutate(date = as.Date(with(., paste(year, month, day,sep="-")), "%Y-%m-%d")) %>%
  mutate(julianDate = yday(date)) 

f10_dates
```
%>%
  drop_na(day) %>%
  drop_na(thaw_depth) %>%
  drop_na(lat) %>%
  drop_na(long) %>%
  dplyr::select(-fire_id, -organic_depth, -slope, -topo_position, -surface_water, -fire_year) 



```{r}
unique(f10_dates$site_id)
```
```{r}
f10_dates %>% group_by(thaw_active, site_id) %>% count(.)
```
There are 732 measurements as thaw depth from site_id YKD

```{r}
f10_dates %>% filter(thaw_active == "T")
```
# Fix longitude missing negative issue
```{r}
( f10_long_fix = f10_dates %>% filter(long > 0) %>% mutate(across(long, ~ .x*(-1))) )
```
Grab existing negative longitude points
```{r}
( f10_long_correct = f10_dates %>% filter(long < 0) )
```
Combine correct and fixed longitude
```{r}
( f10_all_long = f10_long_correct %>% bind_rows(f10_long_fix) )
```

```{r}
f10_all_long %>% distinct(site_id, plot_id, fire_id, burn_unburn) %>% write.csv(., '../outputs/pairs/natali.csv')
```
```{r}
f10_all_long %>% filter(site_id =="YKD")
```

```{r}
f10_all_long %>% distinct(julianDate, year, date)
```
```{r}
f10_all_long %>% filter_all(any_vars(is.na(.))) 
```

# Split by active & thaw
```{r}
f10_all = f10_all_long

f10_thaw = f10_all_long %>% 
  filter(thaw_active == "T") %>%
  drop_na(long) %>%
  drop_na(lat)

f10_active = f10_all_long %>% 
  filter(thaw_active == "A") %>%
  drop_na(long) %>%
  drop_na(lat)
```

```{r}
unique(f10_active$date)
```
Aug 2
Sept 6
Sept 15, 16, 17, 18, 19

```{r}
unique(f10_thaw$date)
```

```{r}
unique(f10_active$julianDate)
```

```{r}
unique(f10_thaw$date)
```

```{r}
unique(f10_thaw$julianDate)
```

```{r}
f10_active %>% 
  filter(is.na(long))

f10_active %>% 
  filter(is.na(lat))
```

# Save clean data
```{r}
write_csv(f10_active, "../outputs/CleanActive/NataliCleanActive.csv")
```

# Create points shapefile

## For f10
sf::st_as_sf(dd, coords = c("x","y"))
```{r}
f10_thaw_pts = st_as_sf(f10_thaw, coords = c("long","lat"), crs = 4326, remove = FALSE)
f10_active_pts = st_as_sf(f10_active, coords = c("long","lat"), crs = 4326, remove = FALSE)
f10_pts = st_as_sf(f10_all, coords = c("long","lat"), crs = 4326, remove = FALSE)
```


### write to shapefile
```{r}
st_write(f10_thaw_pts, "../outputs/ThawPoints/Natali_thaw_pts.shp", driver="ESRI Shapefile")
st_write(f10_active_pts, "../outputs/ActivePoints/Natali_active_pts.shp", driver="ESRI Shapefile")
st_write(f10_pts, "../outputs/allPoints/Natali_pts.shp", driver="ESRI Shapefile")
```