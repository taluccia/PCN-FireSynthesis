---
title: "DataCleaning"
author: "Anna Talucci"
date: '2022-07-19'
output: html_document
---
# clear environment
```{r}
rm(list=ls())
```

# Overview

Adapt script form M. Loranty data_cleaning, to only clean and not aggregate data. 

Use data from csv files in Google Drive

What this script does:


# Packages

```{r}
library(tidyverse)
library(lubridate)
```

# Data

# vector of data files
```{r}
f <- list.files(path = "../data/GoogleDriveCsvFolder/",
                pattern = "*.csv", full.names = TRUE)
```

```{r}
f
```
## define functions

 get the mode of a vector (works for character)
```{r}
getmode <- function(x){
  uv <- unique(x)
  uv[which.max(tabulate(match(x, uv)))]
}
```
calculate standard error of the mean
```{r}
se <- function(x){
  sd(x, na.rm = T)/sqrt(length(na.omit(x)))
}
```


# CLEAN UP AND CONCATENATE INDIVIDUAL FILES 

## Baillargeon
```{r}
f1 <- read.csv(f[1], header = TRUE) 

f1$organic_depth <- 25 # change organic depth range to numeric

f1$gt_probe[which(f1$thaw_depth == "100+")] <- "y" # indicate measurements that exceed probe length

f1$thaw_depth <- as.numeric(sub("100+", "100", f1$thaw_depth, fixed = TRUE))# get rid of non-numeric thaw probe measurements

f1$fire_year <- as.numeric(sub("unburned", NA, f1$fire_year, fixed = TRUE)) # get rid of non-numeric fire year entries

f1$slope <- read.csv(f[1], header = TRUE, colClasses = "character")[,20] # fix slope, which was read as logical

f1$thaw_active <- "T" # fix thaw_active - these are TD measurements, not ALD

f1 %>% distinct(site_id,year,month,day,fire_id,burn_unburn) # examine unique combinations of identify information for aggregatation

f1a <- f1 %>% group_by(site_id,fire_id,burn_unburn,year) %>%
              summarise(td = mean(thaw_depth, na.rm = T),se = se(thaw_depth), 
                        fire_year = mean(fire_year),
                        lat = mean(lat), long = mean(long),
                        biome = getmode(boreal_tundra),veg = getmode(veg_cover_class), 
                        thaw_active = getmode(thaw_active))

pivot_wider(f1a,
            names_from = c(burn_unburn),
            values_from = c(td,se))

```

## Breen
```{r eval=FALSE, include=FALSE}
f2 <- read.csv(f[2], header = TRUE)
```

## Buma
Note--The Dalton and the Steese are larger "sites," each of which has a lot of plots in them.  But those plots do vary, so don't aggregate.  At the Dalton, there are sites with 0 (unburned), 1 (one fire, which would be 2004/2005 era), 2 fires (which would be 1970's era AND 2004 or 2005), or 3 fires (which would be 1950's, 1970's, and 2004 or 2005.  So if aggregating, what you'd want to do would be to aggregate by those treatments (0, 1, 2, or 3 fires) within the Dalton or Steese "sites."  So, sounds like you'd want to aggregate all the unburned plots at the Dalton, all the 1 burn plots at the Dalton, etc.  I would not aggregate Dalton and Steese plots together, they are functionally different sites (uplands vs. low lands, respectively).
```{r}
f3 <- read.csv(f[3], header = TRUE)[,-17] # read without 17th column, which is redundant to the gt_prob column

f3$thaw_active <- read.csv(f[3], header = TRUE, colClasses = "character")[,20] # fix thaw/active column, which was read as logical

f3$site_id <- paste(f3$site_id, sapply(strsplit(f3$plot,"_"),"[[",2), sep="_" ) # fix site_id for aggregating (see Note below from Buma)

f3$long <- ifelse(f3$long > 0, -f3$long,f3$long) # fix positive longitude values, which are in the wrong hemisphere

f3 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)
```



## Dielman
```{r}
f4 <- read.csv(f[4], header = TRUE)

f4[,c(11,18,20,21)] <- read.csv(f[4], header = TRUE, colClasses = "character")[,c(11,18,20,21)] # fix incorrect logical columns

f4 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)
```
## Douglas
```{r}
f5 <- read.csv(f[5], header = TRUE, na.strings = "N/A")[,-23]

# fix incorrect logical columns
f5$slope <- read.csv(f[5], header = TRUE, colClasses = "character")[,20]
```
# aggregate by site_id and year
f4 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)


## Frost -- 
```{r}
f6 <- read.csv(f[6], header = TRUE, na.strings = "-999")

# fix thaw/active column, which was read as logical
f6$thaw_active <- read.csv(f[6], header = TRUE, colClasses = "character")[,19]

# aggregation unclear
```

## Galgiotti
```{r}
f7 <- read.csv(f[7], header = TRUE)

# set gt_probe
f7$gt_probe <- "n"

# organic depth is missing, but set to NA
f7$organic_depth <- as.numeric(f7$organic_depth)

# aggregate by plot_id
f7 %>% distinct(plot_id,year,month,day,fire_id,burn_unburn)
```

## Manies
```{r}
f8 <- read.csv(f[8], header = TRUE)

# fix incorrect columns
f8$slope <- read.csv(f[8], header = TRUE, colClasses = "character")[,20]

f8$organic_depth <- read.csv(f[8], header = TRUE, na.strings = "unk")[,15]

# aggregate by site & date
f8 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)
```

## Natali
```{r}
f9 <- read.csv(f[9], header = TRUE)

# organic depth is missing, but set to NA
f9$organic_depth <- as.numeric(f9$organic_depth)

# fix incorrect logical columns
f9[,c(20:22)] <- read.csv(f[9], header = TRUE, colClasses = "character")[,c(20:22)]

# remove characters from thaw depth and convert to numeric
f9$thaw_depth <- gsub("+", "", f9$thaw_depth, fixed = TRUE)
f9$thaw_depth <- as.numeric(gsub(">", "", f9$thaw_depth, fixed = TRUE))


# aggregate by site & month
f9 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)
```

## O'Donnell
```{r}
f10 <- read.csv(f[10], header = TRUE)

# remove non-numeric characters from numeric vars
# note we're loosing info on where Organic Layer Thickness is in excess of the entered value
f10$organic_depth <- as.numeric(gsub(">", "",f10$organic_depth))

f10$thaw_depth <- as.numeric(gsub(">", "",f10$thaw_depth))

f10 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)
```

## Gibson/Olefeldt
```{r}
f11 <- read.csv(f[11], header = TRUE)

# fix incorrect logical columns
f11$slope <- read.csv(f[11], header = TRUE, colClasses = "character")[,20]

# remove non-numeric characters from numeric vars
# note we're loosing info on where Organic Layer Thickness is in excess of the entered value
f11$organic_depth <- as.numeric(gsub(">", "",f11$organic_depth))

f11 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)
```


## Paulson
```{r}
f12 <- read.csv(f[12], header = TRUE)
f12
```

```{r}
# fix incorrect logical columns
f12$thaw_active <- read.csv(f[12], header = TRUE, colClasses = "character")[,19]

f12 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)
```


## Rocha
```{r}
f13 <- read.csv(f[13], header = TRUE)

# fix incorrect logical columns
f13$slope <- read.csv(f[13], header = TRUE, colClasses = "character")[,20]

# organic depth is missing, but set to NA
f13$organic_depth <- as.numeric(f13$organic_depth)

# convert thaw depth to numeric - blank cells have a period, and will be converted to NA
f13$thaw_depth <- as.numeric(f13$thaw_depth)

f13 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)
```


##Sizov
```{r}
f14 <- read.csv(f[14], header = TRUE)[,-23]
```

## Veraverbeke
```{r}
f15 <- read.csv(f[15], header = TRUE)

# fix incorrect logical columns
f15$thaw_active <- read.csv(f[15], header = TRUE, colClasses = "character")[,19]

f15 %>% distinct(plot_id,year,month,day,fire_id,burn_unburn)
```

# Assemble data in to single Data frame
## concatenate and clean up all of the raw data 
```{r}
all.td <- rbind(f1,f3,f4,f5,f6,f7[,-23],f8[,-23],f9, f10, f11, f12[,-23], f13[,-23],f15)
```

```{r}
all.td
```
## fix inconsistent spelling/capitalization for burned/unburned
```{r}
all.td$burn_unburn[grep("unb", all.td$burn_unburn, ignore.case = TRUE)] <- "unburned"
all.td$burn_unburn[grep("unb", all.td$burn_unburn, ignore.case = TRUE, invert = TRUE)] <- "burned"
```

## fix inconsistent spelling/capitalization for boreal/tundra
```{r}
all.td$boreal_tundra <- sub("B", "boreal", all.td$boreal_tundra)
all.td$boreal_tundra <- sub("T", "tundra", all.td$boreal_tundra)
```

## convert day  & long to numeric
```{r}
all.td$day <- as.numeric(all.td$day)
all.td$long <- as.numeric(all.td$long)
```

## calculate time since fire
```{r}
all.td$tsf <- all.td$year-all.td$fire_year
```


## set time since fire to 200 for unburned 
```{r}
#all.td$tsf[which(all.td$burn_unburn=="unburned")] <- 200


#all.td$thaw_active[which(all.td$thaw_active=="TRUE")] <- "T"

all.td$thaw_depth[which(all.td$thaw_depth > 500)] <- NA

all.td$thaw_depth[which(all.td$thaw_depth < 0)] <- NA
```


########### Files submitted pre-aggregated to the site level ----
## Breen 
f2 <- read.csv(f[2], header = TRUE)

### fix thaw/active column, which was read as logical
f2$thaw_active <- read.csv(f[2], header = TRUE, colClasses = "character")[,24]

# Additional New Code

```{r}
n_distinct(all.td$last_name)
head(all.td)
```

Make column of MM/DD/YY, add column for Julian DoY, 
```{r}
str(all.td)
```
```{r}
df = all.td %>% 
  mutate(date = as.Date(with(., paste(year, month, day,sep="-")), "%Y-%m-%d")) %>%
  mutate(julianDate = yday(date)) 
```

```{r}
str(df)
```

```{r}
df
```
Min/Max year
```{r}
min(df$year, na.rm = T)
max(df$year, na.rm = T)
```

Min/Max Lat
```{r}
min(df$lat, na.rm = T)
max(df$lat, na.rm = T)
```

Min/Max Long
```{r}
min(df$long, na.rm = T)
max(df$long, na.rm = T)
```