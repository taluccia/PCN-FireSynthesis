---
title: "Paulson Clean to Point"
author: "Anna Talucci"
date: '2022-12-11'
output: html_document
---



# clear environment
```{r}
rm(list=ls())
```

# Overview

Adapt script form M. Loranty data_cleaning, to only clean and not aggregate data. 

Use data from csv files in Google Drive

What this script does:


# Packages

```{r}
library(tidyverse)
library(lubridate)
library(sf)
```
# Projection

WGS 84 need for gee
```{r}
wgs_proj =  "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs "
```

# Data

# vector of data files
```{r}
f <- list.files(path = "../data/GoogleDriveCsvFolder/",
                pattern = "*.csv", full.names = TRUE)
```

```{r}
f
```



## Paulson
```{r}
f13 <- read.csv(f[13], header = TRUE)
f13
```

# fix incorrect logical columns
```{r}
# fix incorrect logical columns
f13$thaw_active <- read.csv(f[13], header = TRUE, colClasses = "character")[,19]

f13 %>% distinct(site_id,year,month,day,fire_id,burn_unburn)

```

```{r}
str(f13)
``` 

```{r}
f13_dates = f13 %>%
  mutate(date = as.Date(with(., paste(year, month, day,sep="-")), "%Y-%m-%d")) %>%
  mutate(julianDate = yday(date)) %>%
  dplyr::select(-notes) 

f13_dates
```

```{r}
f13_dates %>% distinct(site_id, plot_id, fire_id, burn_unburn) %>% write.csv(., '../outputs/pairs/paulson.csv')
```
```{r}
f13_dates %>% distinct(julianDate, year, date)
```

# Split by active & thaw
```{r}
f13_thaw = f13_dates %>% 
  filter(thaw_active == "T") 

f13_thaw

f13_active = f13_dates %>% 
  filter(thaw_active == "A") 

f13_active
```

# Create points shapefile

## For f13
sf::st_as_sf(dd, coords = c("x","y"))
```{r}
f13_thaw_pts = st_as_sf(f13_thaw, coords = c("long","lat"), crs = 4326, remove = FALSE)
f13_pts = st_as_sf(f13_dates, coords = c("long","lat"), crs = 4326, remove = FALSE)
```


### write to shapefile
```{r eval=FALSE, include=FALSE}
st_write(f13_thaw_pts, "../outputs/ThawPoints/Paulson_thaw_pts.shp", driver="ESRI Shapefile")
```

```{r}
st_write(f13_pts, "../outputs/allPoints/Paulson_pts.shp", driver="ESRI Shapefile")
```
